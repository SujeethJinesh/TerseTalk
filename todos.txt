TerseTalk – Follow‑up TODOs (not explicitly covered in RESEARCH_PROPOSAL.md)

Context: These are small, high‑leverage improvements that harden UX and testing
around the current pipeline/baselines without changing research scope.

1) PR‑08S — Smoke CLIs: model selector
   - Add --model=echo|real to scripts/pipeline_smoke.py and scripts/baselines_smoke.py.
   - Default to echo (offline‑safe); real path uses ModelClient.
   - Tests: CLI parses flags; real path skipped unless RUN_REAL_MODEL=1.

2) PR‑09R — Pipeline robustness to model schema failures
   - Wrap Worker/Critic structured calls in try/except inside run_pipeline_once.
   - Record failure fields (e.g., {"worker_error": str, "critic_error": str, "status": "error"}).
   - Ensure memory reset still occurs; density/overflow still computed from manager message.
   - Tests: simulate Instructor exceptions; verify non‑crash + fields present.

3) PR‑09T — Optional tokenizer (tiktoken) for token counts
   - Import‑guarded tiktoken support; fallback to len/4 heuristic.
   - Tests: when absent → heuristic path; when present → counts are non‑negative and consistent.

<We have completed all of the above, but none of the below>

4) PR‑08B — Baselines knobs & minor nits
   - Parameterize max_tokens in run_freeform_once/run_llmlingua_once; surface via smoke CLI.
   - Add brief comment in build_freeform_prompt explaining lone "Question:" line retention.
   - Tests: determinism unchanged with default params; schema intact.

5) PR‑UX — Unified real‑run toggles & env docs
   - Consolidate RUN_REAL_MODEL, TERSETALK_OFFLINE_DATA, RUN_REAL_DATASETS, TERSETALK_DISABLE_LL2 docs.
   - Quick section in README/AGENTS for real‑run setup (Ollama/OpenAI), without making network required.

Note: The evaluation harness and accuracy metrics are planned in RESEARCH_PROPOSAL.md; not tracked here.

6) PR‑12A — Analysis script polish
   - Add docstring to pareto_frontier() clarifying objective (min tokens, max accuracy).
   - Sort caps‑ablation buckets deterministically; warn if all overflow_rate values are NaN.
   - Log discovered run count and skipped dirs to stderr for large trees.
   - Extend smoke test to assert pareto_points_synth.csv exists.

7) PR‑12D — Enrich metrics ingestion (optional, guarded)
   - Optionally parse raw_outputs.jsonl for latency quantiles and tokens/SP distributions.
   - Emit an auxiliary CSV (by_run_ext.csv) instead of bloating by_run.csv.
   - Keep stdlib+numpy only; avoid pandas.

8) PR‑12R — Real tasks & multi‑point fronts
   - Produce runs for hotpotqa and gsm8k (offline shards first; real datasets when available).
   - Generate multiple cap settings for tersetalk (via calibrate_caps or manual grid) to populate ablation figure.
   - Add a short README section with an example analyze command and expected artifacts.

9) PR‑12V — Visualization extensions (later)
   - Add SP vs bytes_on_wire scatter per task.
   - Optional error bars (std or 95% CI) when ≥2 repeats per point; annotate n.
   - Toggle to label frontier points only (reduce clutter on dense plots).

10) PR‑DX — Make targets for fmt/lint
    - Add make fmt (black .) and make lint (ruff check .) per repo guide; keep opt‑in.
    - Document in README/AGENTS and ensure CI (later) uses them.

11) PR‑CI — Headless plots in CI
    - Add a minimal GH Action that runs pytest (matplotlib Agg) and archives figures from the smoke run.
    - Ensure caching and timeouts are conservative.

<We have incorporated all of the above into our research proposal>

12) PR‑14R — Per‑role model selection for agents (Worker/Critic) + timeouts
   - Motivation (evidence): Our quick real runs used Ollama `phi:latest` because `llama3.1:8b` stalled within harness timeouts. Accuracy was low on free‑form with small models. Enabling a stronger or different model for the Critic (and/or Worker) can improve end quality without changing the Manager path.
   - Ollama supports independent request contexts, so we can keep Ollama and still run logically independent agents. What we lack is per‑role model configuration.
   - Changes:
     * Add optional flags to the eval driver: `--worker-model`, `--critic-model` (default to `OLLAMA_MODEL`).
     * Allow distinct `ModelClient` instances per role; wire through `PipelineConfig` or runner parameters.
     * Add per‑role request timeouts configurable via CLI/env to mitigate long TTFT for larger models (e.g., llama3.1:8b).
   - Acceptance:
     * Dry-run unchanged; Echo path unaffected.
     * With `--worker-model=llama3.1:8b --critic-model=codellama:13b-instruct` on Ollama, TerseTalk pipeline completes n=2–3 samples within extended timeout and produces summary.json.
     * Document in AGENTS.md that multiple agent roles can target different models via Ollama; no switch to llama.cpp required unless we need process-level control.
